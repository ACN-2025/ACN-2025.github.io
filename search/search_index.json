{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"USTC \u9ad8\u7ea7\u8ba1\u7b97\u673a\u7f51\u7edc \u8bfe\u7a0b\u4e3b\u9875","text":""},{"location":"#_1","title":"\u8bfe\u7a0b\u4fe1\u606f","text":"<ul> <li>\u8bfe\u7a0b\u540d\u79f0\uff1a\u9ad8\u7ea7\u8ba1\u7b97\u673a\u7f51\u7edc (COMP6103P.01)</li> <li>\u4e0a\u8bfe\u65f6\u95f4\u5730\u70b9\uff1a2~20\u5468 GT-B212: 2(11,12,13)</li> <li>\u4e3b\u8bb2\u6559\u5e08\uff1a\u8d75\u529f\u540d (gmzhao at ustc dot edu dot cn)</li> <li>\u52a9\u6559 (\u90ae\u7bb1\uff1a\u62ec\u53f7\u4e2d\u7684\u7528\u6237\u540d at mail dot ustc dot edu dot cn)<ul> <li>\u6731\u5bb6\u6210 (zhu_jc)</li> <li>\u9093\u7acb\u946b (denglx)</li> <li>\u7530\u4f73\u6797 (jltian)</li> <li>\u674e\u7d2b\u60e0 (lizihui2002)</li> </ul> </li> </ul>"},{"location":"#_2","title":"\u8bfe\u7a0b\u5b89\u6392","text":"\u8bfe\u6b21 \u4e3b\u9898 \u8bba\u6587 PPT 1 fat tree A Scalable, Commodity Data Center Network Architecture FatTree 2 VL2 VL2: A scalable and flexible data center network VL2 3 OpenFlow/SDN OpenFlow Enabling Innovation in Campus Networks OpenFlow 4 B4 B4: Experience with a Globally-Deployed Software Defined WAN B4 5 \u8c03\u5ea6 Hedera: Dynamic Flow Scheduling for Data Center Networks Hedera 6 \u66f4\u65b0 Dynamic Scheduling of Network Updates Dionysus 7 SFC SIMPLE-fying Middlebox Policy Enforcement Using SDN SIMPLE 8 NFV ClickOS and the Art of Network Function Virtualization ClickOS 9 P4 P4: Programming Protocol-Independent Packet Processors P4 10 OVS The Design and Implementation of Open vSwitch OVS 11 MPTCP Design, implementation and evaluation of congestion control for multipath TCP MPTCP 12 DCQCN Congestion Control for Large-Scale RDMA Deployments\u3010SIGCOMM15\u3011 - 13 QUIC The QUIC Transport Protocol_ Design and Internet-Scale Deployment \u3010SIGCOMM17\u3011 - 14 \u4eff\u771f\u5668 NOX- Towards an Operating System for Networks  A network in a laptop: rapid prototyping for software-defined networks - 15 \u4eff\u771f\u5668 - - \u5f85\u5b9a \u5149\u7f51\u7edc Helios: a hybrid electrical/optical switch architecture for modular data centers\u3010SIGCOMM10\u3011 -"},{"location":"#_3","title":"\u53ef\u4f9b\u53c2\u8003\u7684\u5206\u4eab\u8bba\u6587\u5217\u8868","text":"\u5e8f\u53f7 \u8bba\u6587\u540d\u79f0 \u4f1a\u8bae \u65f6\u95f4 1 (\u5df2\u9009) Cassini: Network-Aware Job Scheduling in Machine Learning Clusters NSDI 2024 2 Better Together: Jointly Optimizing ML Collective Scheduling and Execution Planning using Syndicate NSDI 2023 3 (\u5df2\u9009) RDMA over Ethernet for Distributed AI Training at Meta Scale NSDI 2024 4 (\u5df2\u9009) Crux: GPU-Efficient Communication Scheduling for Deep Learning Training SIGCOMM 2024 5 (\u5df2\u9009) Alibaba HPN: A Data Center Network for Large Language Model Training SIGCOMM 2024 6 (\u5df2\u9009) Mooncake: Trading More Storage for Less Computation \u2014 A KVCache-centric Architecture for Serving LLM Chatbot FAST 2025 7 SimAI: Unifying Architecture Design and Performance Tuning for Large-Scale Large Language Model Training with Scalability and Precision NSDI 2025 8 (\u5df2\u9009) Efficient Memory Management for Large Language Model Serving with PagedAttention SOSP 2023 9 (\u5df2\u9009) ReCycle: Resilient Training of Large DNNs using Pipeline Adaptation SOSP 2023 10 Themis: A Network Bandwidth-Aware Collective Scheduling Policy for Distributed Training of DL Models ISCA 2022 11 (\u5df2\u9009) Gemini: Fast Failure Recovery in Distributed Training with In-Memory Checkpoint SOSP 2023 12 (\u5df2\u9009) Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models SIGCOMM 2023 13 (\u5df2\u9009) Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention ATC 2024 14 Towards Domain-Specific Network Transport for Distributed DNN Training NSDI 2024 15 (\u5df2\u9009) Differential Network Analysis (DNA) NSDI 2022 16 Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs NSDI 2023 17 One-Size-Fits-None: Understanding and Enhancing Slow-Fault Tolerance in Modern Distributed Systems NSDI 2025 18 Starvation in End-to-End Congestion Control SIGCOMM 2022 19 DUNE: Distributed Inference in the User Plane INFOCOM 2025 20 Nezha: SmartNIC-based Virtual Switch Load Sharing SIGCOMM 2025 21 (\u5df2\u9009) Fast Algorithms for Loop-Free Network Updates using Linear Programming and Local Search INFOCOM 2024 22 Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning ASPLOS 2024 23 DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads ASPLOS 2023 24 (\u5df2\u9009) MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs NSDI 2024 25 TopoopT: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs NSDI 2023 26 White-Boxing RDMA with Packet-Granular Software Control NSDI 2025 27 Unlocking ECMP Programmability for Precise Traffic Control NSDI 2025 28 (\u5df2\u9009) Autellix: An Efficient Serving Engine for LLM Agents as General Programs NSDI 2026 29 Load Balancing With Multi-Level Signals for Lossless Datacenter Networks ToN 2024 30 (\u5df2\u9009) Swing: Short-cutting Rings for Higher Bandwidth Allreduce NSDI 2024 31 Rethinking Machine Learning Collective Communication as a Multi-Commodity Flow Problem SIGCOMM 2024 32 MCCS: A Service-based Approach to Collective Communication for Multi-Tenant Cloud SIGCOMM 2024 33 Whale: Efficient Giant Model Training over Heterogeneous GPUs ATC 2023 34 (\u5df2\u9009) Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve OSDI 2024 35 (\u5df2\u9009) ServerlessLLM: Low-Latency Serverless Inference for Large Language Models OSDI 2024 36 Parcae: Proactive, Liveput-Optimized DNN Training on Preemptible Instances NSDI 2024 37 (\u5df2\u9009) CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving SIGCOMM 2024"},{"location":"#presentation","title":"\u5b66\u751f Presentation \u5b89\u6392","text":"\u65f6\u95f4 \u6c47\u62a5\u4eba \u8bba\u6587 \u7b2c3\u5468 2025.9.23 \u5510\u6893\u7693 \u9b4f\u6e05\u626c Crux: GPU-Efficient Communication Scheduling for Deep Learning Training \u7b2c4\u5468 2025.9.30 \u6c88\u5609\u73ae \u674e\u5b87\u822a Alibaba HPN: A Data Center Network for Large Language Model Training \u7b2c6\u5468 2025.10.14 \u674e\u5b87\u54f2 \u5218\u777f\u535a \u5468\u74ef\u7fd4 AutoCCL: Automated Collective Communication Tuning for Accelerating Distributed and Parallel DNN Training \u7b2c7\u5468 2025.10.21 \u5468\u6656\u6797 \u5218\u56fd\u67f1 \u6768\u654f Efficient Memory Management for Large Language Model Serving with PagedAttention \u7b2c8\u5468 2025.10.28 \u80e1\u6f47\u9038 \u8d75\u82f1\u8c6a \u6731\u709c\u8363 Swing: Short-cutting Rings for Higher Bandwidth Allreduce \u7b2c9\u5468 2025.11.4 \u9648\u6da6\u4f73 \u5218\u76c8\u777f \u5218\u73c8\u8fb0 Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention \u7b2c10\u5468 2025.11.11 \u968b\u7fca \u5f20\u827e\u5a9b \u8fb9\u950b Mooncake:Trading More Storage for Less Computation -A KVCache-centric Architecture for Serving LLMChatbot \u7b2c11\u5468 2025.11.18 \u6885\u9676\u7136 \u53f2\u5f18\u4f50 \u9ec4\u4e07\u8d85 Janus: A Unified Distributed Training Framework for Sparse Mixture-of-Experts Models \u7b2c12\u5468 2025.11.25 \u5218\u4e9a\u56fd \u5f20\u6866\u575a \u738b\u9053\u5b87 Autellix: An Efficient Serving Engine for LLM Agents as General Programs \u7b2c13\u5468 2025.12.02 \u5434\u6653\u6625 \u6c6a\u5ef6 \u65f6\u9510 RDMA over Ethernet for Distributed Al Training at Meta Scale \u7b2c14\u5468 2025.12.09 \u9a6c\u5609\u6167 \u9773\u742a Differential Network Analysis (DNA) \u7b2c15\u5468 2025.12.16 \u8303\u51b0 \u80e1\u6052\u745e \u9648\u667a\u6167 CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving \u5f20\u5b87\u6d0b \u5b8b\u5b50\u9633 \u6c88\u6960 How to Disturb Network Reconnaissance: A Moving Target Defense Approach Based on Deep Reinforcement Learning \u9648\u4e16\u521d \u5f20\u6052\u57fa \u65b9\u9a70\u6b63 Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve \u7b2c16\u5468 2025.12.23 \u675c\u67cf\u8a00 \u5434\u98ce\u5e06 \u6797\u6587\u6d69 MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs \u9ec4\u4f73\u4f9d \u8d75\u6db5 \u82d7\u660e\u946b ReCycle: Resilient Training of Large DNNs using Pipeline Adaptation \u674e\u5cb1\u9716 \u5218\u7fd4\u5b87 \u924f\u535a\u6d0b Gemini: Fast Failure Recovery in Distributed Training with In-Memory Checkpoint \u7b2c17\u5468 2025.12.30 \u9648\u5b50\u9633 \u4e01\u5219\u6587 \u5019\u4e16\u5353 ServerlessLLM: Low-Latency Serverless Inference for Large Language Models \u6c5f\u4f1f\u6021 \u80e1\u68a6\u5a77 \u848b\u96e8\u542b CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters \u738b\u82e5\u8a00 \u5218\u4e30\u6bc5 Fast Algorithms for Loop-Free Network Updates using Linear Programming and Local Search"}]}